{
    "nbformat_minor": 0, 
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }, 
        "celltoolbar": "Raw Cell Format"
    }, 
    "cells": [
        {
            "source": "Data analysis, also known as analysis of data or data analytics, is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.\nData integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.\n", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "raw"
        }, 
        {
            "source": "# The process of data analysis", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Data requirements\nThe data necessary as inputs to the analysis are specified based upon the requirements of those directing the analysis or customers who will use the finished product of the analysis. The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained. Data may be numerical or categorical (i.e., a text label for numbers). ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Data collection\nData is collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data, such as information technology personnel within an organization. The data may also be collected from sensors in the environment, such as traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Data processing\nThe phases of the intelligence cycle used to convert raw information into actionable intelligence or knowledge are conceptually similar to the phases in data analysis.\nData initially obtained must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (i.e., structured data) for further analysis, such as within a spreadsheet or statistical software. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Data cleaning\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that data is entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable. Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of data cleaning that depend on the type of data such as phone numbers, email addresses, employers etc. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spellcheckers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Exploratory data analysis\nOnce the data is cleaned, it can be analyzed. Analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained in the data. The process of exploration may result in additional data cleaning or additional requests for data, so these activities may be iterative in nature. Descriptive statistics such as the average or median may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Modeling and algorithms\nMathematical formulas or models called algorithms may be applied to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some residual error depending on model accuracy (i.e., Data = Model + Error). \n\nInferential statistics includes techniques to measure relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X) explains the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as Y = aX + b + error, where the model is designed such that a and b minimize the error when the model predicts Y for a given range of values of X. Analysts may attempt to build models that are descriptive of the data to simplify analysis and communicate results. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Data product\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. An example is an application that analyzes data about customer purchasing history and recommends other purchases the customer might enjoy. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## Communication\nData visualization to understand the results of a data analysis. \nOnce the data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative. \nWhen determining how to communicate the results, the analyst may consider data visualization techniques to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays such as tables and charts to help communicate key messages contained in the data. Tables are helpful to a user who might lookup specific numbers, while charts (e.g., bar charts or line charts) may help explain the quantitative messages contained in the data.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Data Analytics Tools - Data Visualisation", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Name\t   Visual Dimensions\t    Example Usages\n    \n    Bar chart    length/count             Comparison of values, such as sales performance for several persons or businesses in a single time period.                  category                 For a single variable measured over time (trend) a line chart is preferable.\n                 (color)\n\n    Histogram\t bin limits               Determining frequency of annual stock market percentage returns within particular ranges (bins) such as 0-                  count/length             10%, 11-20%, etc. The height of the bar represents the number of observations (years) with a return % in                    (color)                  the range represented by the bin.\n\n    Scatter plot x-position               Determining the relationship (e.g., correlation) between unemployment (x) and inflation (y) for multiple                    y-position               time periods.\n                 (symbol/glyph)\n                 (color)\n                 (size)\n\n    Network\t     nodes size               Finding clusters in the network (e.g. grouping Facebook friends into different clusters).\n                 nodes color              Discovering bridges (information brokers or boundary spanners) between clusters in the network\n                 ties thickness           Determining the most influential nodes in the network (e.g. A company wants to target a small group of                      ties color               people on Twitter for a marketing campaign).\n                 spatialization           Finding outlier actors who does not fit in any cluster or in the periphery of a network.\n\n    Streamgraph\t width                    \n                 color\n                 time (flow)\n\n    Treemap\t     size                     disk space by location / file type\n                 color\n\n\n    Gantt chart\t color                    schedule / progress, e.g. in project planning\n                 time (flow)\n\n\n    Heat map\t row                     Analyzing risk, with green, yellow and red representing low, medium, and high risk, respectively.\n                 column\n                 cluster\n                 color", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4
}